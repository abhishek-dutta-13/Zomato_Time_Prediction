{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "from src.utils import *\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-18 16:06:58 - Zomato_Time_Prediction - utils - INFO : Connecting to MySQL...\n",
      "2024-03-18 16:06:58 - Zomato_Time_Prediction - utils - INFO : MySQL connection established\n",
      "2024-03-18 16:06:59 - Zomato_Time_Prediction - 272529110 - INFO : MySQL connection closed\n"
     ]
    }
   ],
   "source": [
    "# Extraction of Data:\n",
    "try:\n",
    "    conn, cursor = mysql_connection()\n",
    "    sql = \"select * from zomato.delivery\"\n",
    "    cursor.execute(sql)\n",
    "    result = cursor.fetchall()\n",
    "    df = pd.DataFrame(result)\n",
    "except Exception as e:\n",
    "    logger.error(\"Error extracting data from MySQL\")\n",
    "    raise CustomException(e, sys)\n",
    "finally:\n",
    "    conn.close()\n",
    "    logger.info(\"MySQL connection closed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns:\n",
    "drop_cols = ['SerialNo', 'ID', 'Delivery_person_ID', 'Order_Date', 'Time_Orderd', 'Time_Order_picked' ]\n",
    "df.drop(labels=drop_cols, axis=1, inplace=True)\n",
    "\n",
    "#Calculating distance:\n",
    "df[\"distance\"] = df.apply(lambda row: cal_distance(row['Restaurant_latitude'], row['Restaurant_longitude'], row['Delivery_location_latitude'], row['Delivery_location_longitude']), axis=1)\n",
    "df.drop(labels=[ 'Restaurant_latitude', 'Restaurant_longitude', 'Delivery_location_latitude', 'Delivery_location_longitude'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = []\n",
    "num_cols = []\n",
    "\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'object':\n",
    "        cat_cols.append(i)\n",
    "    else:\n",
    "        num_cols.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fill_empty_with_mode(df,cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.63658376810238"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Overall mean\n",
    "mean = df[\"Time_taken (min)\"].mean()\n",
    "thereshold_percentage = 0.1\n",
    "threshold_value = mean * thereshold_percentage\n",
    "threshold_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5996474791257365"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_weather = df[[\"Weather_conditions\", \"Time_taken (min)\"]].groupby(\"Weather_conditions\").mean()\n",
    "df_weather[\"Time_taken (min)\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.054263656149913"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_road = df[[\"Road_traffic_density\", \"Time_taken (min)\"]].groupby(\"Road_traffic_density\").mean()\n",
    "df_road[\"Time_taken (min)\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08766091041580491"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_road = df[[\"Type_of_order\", \"Time_taken (min)\"]].groupby(\"Type_of_order\").mean()\n",
    "df_road[\"Time_taken (min)\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5038591080461499"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_road = df[[\"Type_of_vehicle\", \"Time_taken (min)\"]].groupby(\"Type_of_vehicle\").mean()\n",
    "df_road[\"Time_taken (min)\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.79609904522959"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_road = df[[\"Festival\", \"Time_taken (min)\"]].groupby(\"Festival\").mean()\n",
    "df_road[\"Time_taken (min)\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.329871133322866"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_road = df[[\"City\", \"Time_taken (min)\"]].groupby(\"City\").mean()\n",
    "df_road[\"Time_taken (min)\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Vehicle_condition\"] = df[\"Vehicle_condition\"].replace(0,1)\n",
    "df[\"multiple_deliveries\"] = df[\"multiple_deliveries\"].replace(0.0,1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing ordinal and onehot encoder:\n",
    "\n",
    "1. Low Standard Deviation: If the standard deviation of the mean time taken across categories is low (e.g., less than 10% of the overall mean of the response variable), it suggests that the mean time taken doesn't vary much between categories. In such cases, OneHotEncoder might be suitable, especially if the categorical variable is nominal.\n",
    "\n",
    "2. High Standard Deviation: If the standard deviation is high (e.g., greater than 10% of the overall mean), it indicates significant variability in the mean time taken between categories. For ordinal variables or when you want to capture this variability without increasing dimensionality too much, OrdinalEncoder might be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureClassifier:\n",
    "    def __init__(self,df, target_column):\n",
    "        self.df = df\n",
    "        self.target_column = target_column\n",
    "    \n",
    "    def get_ordinal_columns_mapping(self,columns):\n",
    "        \"\"\"\n",
    "        This function is used to get the mapping of ordinal columns.\n",
    "        Each key is named as 'ColumnName_Map' and contains the unique values for that column.\n",
    "        \"\"\"\n",
    "        ordinal_columns_mapping = {}\n",
    "        for col in columns:\n",
    "            sorted_groups = self.df.groupby(col)[self.target_column].mean().sort_values().index.tolist()\n",
    "            key_name = f\"{col}_Map\"\n",
    "            ordinal_columns_mapping[key_name] = sorted_groups\n",
    "        \n",
    "        return ordinal_columns_mapping\n",
    "        \n",
    "\n",
    "        \n",
    "    def ordinal_onehot_numerical_divide(self):\n",
    "        \"\"\"\n",
    "        This function is used to divide the categorical into ordinal and one-hot columns and numerical columns.\n",
    "        \"\"\"\n",
    "        one_hot_cols = []\n",
    "        ordinal_cols = []\n",
    "        num_cols = []\n",
    "        #Overall mean\n",
    "        mean = df[self.target_column].mean()\n",
    "        thereshold_percentage = 0.1\n",
    "        threshold_value = mean * thereshold_percentage\n",
    "        try:\n",
    "            for column in self.df.columns:\n",
    "                if column != self.target_column and self.df[column].dtype == 'object':\n",
    "                    df_column = self.df[[column, self.target_column]].groupby(column).mean().reset_index()\n",
    "                    standard_dev = df_column[self.target_column].std()\n",
    "                    if standard_dev > threshold_value:\n",
    "                        ordinal_cols.append(column)\n",
    "                    else:\n",
    "                        one_hot_cols.append(column)\n",
    "                else:\n",
    "                    num_cols.append(column)\n",
    "\n",
    "            #Get Mappingsd for ordinal columns:\n",
    "            ordinal_columns_mapping = self.get_ordinal_columns_mapping(ordinal_cols)\n",
    "            return (one_hot_cols, ordinal_cols, num_cols, ordinal_columns_mapping)\n",
    "                 \n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise CustomException(\"Error in feature_classifier.ordinal_onehot_numerical_divide: {}\".format(e, sys))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = \"Time_taken (min)\"\n",
    "feature_classifier_obj = FeatureClassifier(df, target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_cols, ordinal_cols, num_cols, ordinal_columns_mapping = feature_classifier_obj.ordinal_onehot_numerical_divide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Weather_conditions', 'Type_of_order', 'Type_of_vehicle'],\n",
       " '\\n',\n",
       " ['Road_traffic_density', 'Festival', 'City'],\n",
       " '\\n',\n",
       " ['Delivery_person_Age',\n",
       "  'Delivery_person_Ratings',\n",
       "  'Vehicle_condition',\n",
       "  'multiple_deliveries',\n",
       "  'Time_taken (min)',\n",
       "  'distance'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_cols, \"\\n\", ordinal_cols, \"\\n\", num_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Road_traffic_density_Map': ['Low', 'Medium', 'High', 'Jam'],\n",
       " 'Festival_Map': ['No', 'Yes'],\n",
       " 'City_Map': ['Urban', 'Metropolitian', 'Semi-Urban']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinal_columns_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier Removal:\n",
    "df_filter = outlier_removal(df, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(df_filter, test_size=0.2, random_state=42)\n",
    "\n",
    "input_feature_train_df = train_set.drop(labels=target_column,axis=1)\n",
    "input_feature_test_df = test_set.drop(labels=target_column,axis=1)\n",
    "target_training_df = train_set[target_column]\n",
    "target_testing_df = test_set[target_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Low', 'Medium', 'High', 'Jam'],\n",
       " ['No', 'Yes'],\n",
       " ['Urban', 'Metropolitian', 'Semi-Urban']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = []\n",
    "for key, value in ordinal_columns_mapping.items():\n",
    "    categories.append(value)\n",
    "categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "num_cols_pipe = [col for col in num_cols if col != \"Time_taken (min)\"]\n",
    "\n",
    "# Define pipelines for categorical and numeric data\n",
    "categorical_onehot_pipeline = Pipeline([\n",
    "    \n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(sparse_output=False)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_ordinal_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OrdinalEncoder(categories=categories)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine pipelines in a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('cat_one_hot', categorical_onehot_pipeline, one_hot_cols),\n",
    "    ('cat_ordinal', categorical_ordinal_pipeline, ordinal_cols),\n",
    "    ('num', numerical_pipeline, num_cols_pipe)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_training_arr = preprocessor.fit_transform(input_feature_train_df)\n",
    "input_testing_arr = preprocessor.transform(input_feature_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr = np.c_[input_training_arr, np.array(target_training_df)]\n",
    "test_arr = np.c_[input_testing_arr, np.array(target_testing_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = (\n",
    "    train_arr[:, :-1],\n",
    "    train_arr[:, -1],\n",
    "    test_arr[:, :-1],\n",
    "    test_arr[:, -1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge,Lasso,ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor\n",
    "\n",
    "models = {\n",
    "    'LinearRegression':LinearRegression(),\n",
    "    'Lasso':Lasso(),\n",
    "    'Ridge':Ridge(),\n",
    "    'Elasticnet':ElasticNet(),\n",
    "    'SVR': SVR(),\n",
    "    'DecisionTree':DecisionTreeRegressor(),\n",
    "    'RandomForest':RandomForestRegressor(),\n",
    "    'GradientBoostingRegressor':GradientBoostingRegressor(),\n",
    "    'BaggingRegressor' : BaggingRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config_path = \"../params.yaml\"\n",
    "\n",
    "#Load yaml file:\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "for model_name, model_instance in models.items():\n",
    "    if model_name != \"LinearRegression\":\n",
    "        model_params = config[model_name]\n",
    "        random_cv, best_params_, best_score_ = random_search_cv(model_instance, X_train, y_train, model_params)\n",
    "        print(\"#\"*80, \"\\n\")\n",
    "        print(f\"{model_name}:\\n\")\n",
    "        print(f\"Training Score: {best_score_}\\n\")\n",
    "        print(f\"Best Params: {best_params_}\\n\")\n",
    "        y_pred = random_cv.predict(X_test)\n",
    "        r2_score_value = r2_score(y_test, y_pred)\n",
    "        print(f\"Testing Score: {r2_score_value}\\n\\n\\n\\n\")\n",
    "    else:\n",
    "        model = model_instance\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        r2_score_value = r2_score(y_test, y_pred)\n",
    "        print(\"#\"*80, \"\\n\")\n",
    "        print(f\"{model_name}:\\n\")\n",
    "        print(f\"Testing Score: {r2_score_value}\\n\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
